{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDream\n",
    "DeepDream is an artistic image-modification technique that uses the representations learned by convolutional neural networks. It was first released by Google in the summer of 2015, as an implementation written using the Caffe deep-learning library (this\n",
    "was several months before the first public release of TensorFlow).4 It quickly became an internet sensation thanks to the trippy pictures it could generate, full of algorithmic pareidolia artifacts, bird feathers, and dog eyes—a byproduct of the fact that the DeepDream convnet was trained on ImageNet, where dog breeds and bird species are vastly overrepresented.\n",
    "\n",
    "The DeepDream algorithm is almost identical to the convnet filter-visualization technique, consisting of running a convnet in reverse: doing gradient ascent on the input to the convnet in order to maximize the activation of a\n",
    "specific filter in an upper layer of the convnet. DeepDream uses this same idea, with a few simple differences:\n",
    " - With DeepDream, we try to maximize the activation of entire layers rather than that of a specific filter, thus mixing together visualizations of large numbers of features at once.\n",
    " - We start not from blank, slightly noisy input, but rather from an existing image—thus the resulting effects latch on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.\n",
    " - The input images are processed at different scales (called *octaves*), which improves the quality of the visualizations.\n",
    " \n",
    "\n",
    "### Implement DeepDream in Keras\n",
    "We’ll start from a convnet pretrained on ImageNet. In Keras, many such convnets are available: `VGG16`, `VGG19`, `Xception`, `ResNet50`, and so on. You can implement Deep- Dream with any of them, but your convnet of choice will naturally affect your visualizations, because different convnet architectures result in different learned features. The convnet used in the original DeepDream release was an Inception model, and in practice Inception is known to produce nice-looking DeepDreams, so we’ll use the Inception V3 model that comes with Keras.\n",
    "\n",
    "### Load The Pretrained Inception V3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 79s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)  #We won’t be training the model, so this command disables all trainingspecific operations.\n",
    "\n",
    "model= inception_v3.InceptionV3(weights='imagenet', include_top=False) #Builds the Inception V3 network, without its \n",
    "                                                                       #convolutional base. The model will be loaded with\n",
    "                                                                       #pretrained ImageNet weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll compute the loss: the quantity we’ll seek to maximize during the gradient-ascent process. For filter visualization, we tried to maximize the value of a specific filter in a specific layer. Here, we’ll simultaneously maximize the activation of all filters in a number of layers. Specifically, we’ll maximize a weighted sum of the L2 norm of the activations of a set of high-level layers. The exact set of layers you choose (as well as their contribution to the final loss) has a major influence on the visuals we’ll be able to produce, so we want to make these parameters easily configurable. Lower layers result in geometric patterns, whereas higher layers result in visuals in which we can recognize some classes from ImageNet (for example, birds or\n",
    "dogs). We’ll start from a somewhat arbitrary configuration involving four layers—but we’ll definitely want to explore many different configurations later.\n",
    "\n",
    "### Set Up The DeepDream Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions={          #Dictionary mapping layer names to a coefficient quantifying how much the layer’s \n",
    "    'mixed2': 0.2,            #activation contributes to the loss you’ll seek to maximize. Note that the layer names are\n",
    "    'mixed3': 3.0,            #hardcoded in the built-in Inception V3 application. We can list all layer names using \n",
    "    'mixed4': 2.0,            #model.summary().\n",
    "    'mixed5': 1.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s define a tensor that contains the loss: the weighted sum of the L2 norm of the activations of the layers in the above listing.\n",
    "\n",
    "### Define The Loss to be Maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_1': <keras.engine.input_layer.InputLayer at 0x1f76fd74898>,\n",
       " 'conv2d_1': <keras.layers.convolutional.Conv2D at 0x1f76fd74a58>,\n",
       " 'batch_normalization_1': <keras.layers.normalization.BatchNormalization at 0x1f76fd74a20>,\n",
       " 'activation_1': <keras.layers.core.Activation at 0x1f76fd74e10>,\n",
       " 'conv2d_2': <keras.layers.convolutional.Conv2D at 0x1f76fd9b550>,\n",
       " 'batch_normalization_2': <keras.layers.normalization.BatchNormalization at 0x1f76fe0d8d0>,\n",
       " 'activation_2': <keras.layers.core.Activation at 0x1f76fe0d668>,\n",
       " 'conv2d_3': <keras.layers.convolutional.Conv2D at 0x1f707555518>,\n",
       " 'batch_normalization_3': <keras.layers.normalization.BatchNormalization at 0x1f707593f60>,\n",
       " 'activation_3': <keras.layers.core.Activation at 0x1f7075ba7b8>,\n",
       " 'max_pooling2d_1': <keras.layers.pooling.MaxPooling2D at 0x1f7078e5470>,\n",
       " 'conv2d_4': <keras.layers.convolutional.Conv2D at 0x1f7078b2e80>,\n",
       " 'batch_normalization_4': <keras.layers.normalization.BatchNormalization at 0x1f7084fd828>,\n",
       " 'activation_4': <keras.layers.core.Activation at 0x1f7084fdfd0>,\n",
       " 'conv2d_5': <keras.layers.convolutional.Conv2D at 0x1f7084fd9b0>,\n",
       " 'batch_normalization_5': <keras.layers.normalization.BatchNormalization at 0x1f708567978>,\n",
       " 'activation_5': <keras.layers.core.Activation at 0x1f708567b00>,\n",
       " 'max_pooling2d_2': <keras.layers.pooling.MaxPooling2D at 0x1f70b71d6d8>,\n",
       " 'conv2d_9': <keras.layers.convolutional.Conv2D at 0x1f70df6d7f0>,\n",
       " 'batch_normalization_9': <keras.layers.normalization.BatchNormalization at 0x1f70eab3b38>,\n",
       " 'activation_9': <keras.layers.core.Activation at 0x1f70eab35c0>,\n",
       " 'conv2d_7': <keras.layers.convolutional.Conv2D at 0x1f70da56438>,\n",
       " 'conv2d_10': <keras.layers.convolutional.Conv2D at 0x1f70eb0dcc0>,\n",
       " 'batch_normalization_7': <keras.layers.normalization.BatchNormalization at 0x1f70da7afd0>,\n",
       " 'batch_normalization_10': <keras.layers.normalization.BatchNormalization at 0x1f70eb75f98>,\n",
       " 'activation_7': <keras.layers.core.Activation at 0x1f70da9d128>,\n",
       " 'activation_10': <keras.layers.core.Activation at 0x1f70eb75b70>,\n",
       " 'average_pooling2d_1': <keras.layers.pooling.AveragePooling2D at 0x1f714bfd5f8>,\n",
       " 'conv2d_6': <keras.layers.convolutional.Conv2D at 0x1f70b71db00>,\n",
       " 'conv2d_8': <keras.layers.convolutional.Conv2D at 0x1f70daf5860>,\n",
       " 'conv2d_11': <keras.layers.convolutional.Conv2D at 0x1f70ec29eb8>,\n",
       " 'conv2d_12': <keras.layers.convolutional.Conv2D at 0x1f714bfda20>,\n",
       " 'batch_normalization_6': <keras.layers.normalization.BatchNormalization at 0x1f70da2fda0>,\n",
       " 'batch_normalization_8': <keras.layers.normalization.BatchNormalization at 0x1f70dee66a0>,\n",
       " 'batch_normalization_11': <keras.layers.normalization.BatchNormalization at 0x1f70ec66fd0>,\n",
       " 'batch_normalization_12': <keras.layers.normalization.BatchNormalization at 0x1f714c66828>,\n",
       " 'activation_6': <keras.layers.core.Activation at 0x1f756d06a58>,\n",
       " 'activation_8': <keras.layers.core.Activation at 0x1f70dee6c18>,\n",
       " 'activation_11': <keras.layers.core.Activation at 0x1f70ec66a20>,\n",
       " 'activation_12': <keras.layers.core.Activation at 0x1f714cc0ef0>,\n",
       " 'mixed0': <keras.layers.merge.Concatenate at 0x1f714cf06d8>,\n",
       " 'conv2d_16': <keras.layers.convolutional.Conv2D at 0x1f714f88780>,\n",
       " 'batch_normalization_16': <keras.layers.normalization.BatchNormalization at 0x1f714fc6f60>,\n",
       " 'activation_16': <keras.layers.core.Activation at 0x1f714ff1080>,\n",
       " 'conv2d_14': <keras.layers.convolutional.Conv2D at 0x1f714dd6518>,\n",
       " 'conv2d_17': <keras.layers.convolutional.Conv2D at 0x1f7150487f0>,\n",
       " 'batch_normalization_14': <keras.layers.normalization.BatchNormalization at 0x1f714e16860>,\n",
       " 'batch_normalization_17': <keras.layers.normalization.BatchNormalization at 0x1f7150b9978>,\n",
       " 'activation_14': <keras.layers.core.Activation at 0x1f714e16e48>,\n",
       " 'activation_17': <keras.layers.core.Activation at 0x1f7150b9b00>,\n",
       " 'average_pooling2d_2': <keras.layers.pooling.AveragePooling2D at 0x1f715228588>,\n",
       " 'conv2d_13': <keras.layers.convolutional.Conv2D at 0x1f714cf07b8>,\n",
       " 'conv2d_15': <keras.layers.convolutional.Conv2D at 0x1f714e955f8>,\n",
       " 'conv2d_18': <keras.layers.convolutional.Conv2D at 0x1f7151c8320>,\n",
       " 'conv2d_19': <keras.layers.convolutional.Conv2D at 0x1f715228a90>,\n",
       " 'batch_normalization_13': <keras.layers.normalization.BatchNormalization at 0x1f714d26dd8>,\n",
       " 'batch_normalization_15': <keras.layers.normalization.BatchNormalization at 0x1f714f29128>,\n",
       " 'batch_normalization_18': <keras.layers.normalization.BatchNormalization at 0x1f71517de80>,\n",
       " 'batch_normalization_19': <keras.layers.normalization.BatchNormalization at 0x1f715266b70>,\n",
       " 'activation_13': <keras.layers.core.Activation at 0x1f714d536d8>,\n",
       " 'activation_15': <keras.layers.core.Activation at 0x1f714f60d30>,\n",
       " 'activation_18': <keras.layers.core.Activation at 0x1f715200b38>,\n",
       " 'activation_19': <keras.layers.core.Activation at 0x1f715294668>,\n",
       " 'mixed1': <keras.layers.merge.Concatenate at 0x1f71531c320>,\n",
       " 'conv2d_23': <keras.layers.convolutional.Conv2D at 0x1f7155bd390>,\n",
       " 'batch_normalization_23': <keras.layers.normalization.BatchNormalization at 0x1f7155f86a0>,\n",
       " 'activation_23': <keras.layers.core.Activation at 0x1f71565ff28>,\n",
       " 'conv2d_21': <keras.layers.convolutional.Conv2D at 0x1f7153de7f0>,\n",
       " 'conv2d_24': <keras.layers.convolutional.Conv2D at 0x1f715708320>,\n",
       " 'batch_normalization_21': <keras.layers.normalization.BatchNormalization at 0x1f715447358>,\n",
       " 'batch_normalization_24': <keras.layers.normalization.BatchNormalization at 0x1f7156bfe80>,\n",
       " 'activation_21': <keras.layers.core.Activation at 0x1f715447710>,\n",
       " 'activation_24': <keras.layers.core.Activation at 0x1f715740b38>,\n",
       " 'average_pooling2d_3': <keras.layers.pooling.AveragePooling2D at 0x1f71585ceb8>,\n",
       " 'conv2d_20': <keras.layers.convolutional.Conv2D at 0x1f7152ebda0>,\n",
       " 'conv2d_22': <keras.layers.convolutional.Conv2D at 0x1f7154a3dd8>,\n",
       " 'conv2d_25': <keras.layers.convolutional.Conv2D at 0x1f71576a588>,\n",
       " 'conv2d_26': <keras.layers.convolutional.Conv2D at 0x1f71582beb8>,\n",
       " 'batch_normalization_20': <keras.layers.normalization.BatchNormalization at 0x1f7153bbf60>,\n",
       " 'batch_normalization_22': <keras.layers.normalization.BatchNormalization at 0x1f715509f60>,\n",
       " 'batch_normalization_25': <keras.layers.normalization.BatchNormalization at 0x1f7157a9f98>,\n",
       " 'batch_normalization_26': <keras.layers.normalization.BatchNormalization at 0x1f715897ac8>,\n",
       " 'activation_20': <keras.layers.core.Activation at 0x1f7153791d0>,\n",
       " 'activation_22': <keras.layers.core.Activation at 0x1f715534080>,\n",
       " 'activation_25': <keras.layers.core.Activation at 0x1f7157a9b70>,\n",
       " 'activation_26': <keras.layers.core.Activation at 0x1f734d06128>,\n",
       " 'mixed2': <keras.layers.merge.Concatenate at 0x1f734d60898>,\n",
       " 'conv2d_28': <keras.layers.convolutional.Conv2D at 0x1f734e493c8>,\n",
       " 'batch_normalization_28': <keras.layers.normalization.BatchNormalization at 0x1f734e8beb8>,\n",
       " 'activation_28': <keras.layers.core.Activation at 0x1f734eb2358>,\n",
       " 'conv2d_29': <keras.layers.convolutional.Conv2D at 0x1f734f41390>,\n",
       " 'batch_normalization_29': <keras.layers.normalization.BatchNormalization at 0x1f734f7b6a0>,\n",
       " 'activation_29': <keras.layers.core.Activation at 0x1f734f7bc18>,\n",
       " 'conv2d_27': <keras.layers.convolutional.Conv2D at 0x1f734d60b38>,\n",
       " 'conv2d_30': <keras.layers.convolutional.Conv2D at 0x1f7350057f0>,\n",
       " 'batch_normalization_27': <keras.layers.normalization.BatchNormalization at 0x1f734e20c18>,\n",
       " 'batch_normalization_30': <keras.layers.normalization.BatchNormalization at 0x1f735069b38>,\n",
       " 'activation_27': <keras.layers.core.Activation at 0x1f734dc7048>,\n",
       " 'activation_30': <keras.layers.core.Activation at 0x1f7350c6c50>,\n",
       " 'max_pooling2d_3': <keras.layers.pooling.MaxPooling2D at 0x1f7350ea6a0>,\n",
       " 'mixed3': <keras.layers.merge.Concatenate at 0x1f7350eaba8>,\n",
       " 'conv2d_35': <keras.layers.convolutional.Conv2D at 0x1f73547f4a8>,\n",
       " 'batch_normalization_35': <keras.layers.normalization.BatchNormalization at 0x1f7354c17b8>,\n",
       " 'activation_35': <keras.layers.core.Activation at 0x1f7354c1d30>,\n",
       " 'conv2d_36': <keras.layers.convolutional.Conv2D at 0x1f735542550>,\n",
       " 'batch_normalization_36': <keras.layers.normalization.BatchNormalization at 0x1f7355acb38>,\n",
       " 'activation_36': <keras.layers.core.Activation at 0x1f735606c50>,\n",
       " 'conv2d_32': <keras.layers.convolutional.Conv2D at 0x1f7351df4e0>,\n",
       " 'conv2d_37': <keras.layers.convolutional.Conv2D at 0x1f73562f6a0>,\n",
       " 'batch_normalization_32': <keras.layers.normalization.BatchNormalization at 0x1f73521ee48>,\n",
       " 'batch_normalization_37': <keras.layers.normalization.BatchNormalization at 0x1f73566ec88>,\n",
       " 'activation_32': <keras.layers.core.Activation at 0x1f73521ea58>,\n",
       " 'activation_37': <keras.layers.core.Activation at 0x1f73566ecc0>,\n",
       " 'conv2d_33': <keras.layers.convolutional.Conv2D at 0x1f73529b710>,\n",
       " 'conv2d_38': <keras.layers.convolutional.Conv2D at 0x1f7356f2cc0>,\n",
       " 'batch_normalization_33': <keras.layers.normalization.BatchNormalization at 0x1f73530b400>,\n",
       " 'batch_normalization_38': <keras.layers.normalization.BatchNormalization at 0x1f73575e668>,\n",
       " 'activation_33': <keras.layers.core.Activation at 0x1f735367e48>,\n",
       " 'activation_38': <keras.layers.core.Activation at 0x1f73575ebe0>,\n",
       " 'average_pooling2d_4': <keras.layers.pooling.AveragePooling2D at 0x1f7358d05f8>,\n",
       " 'conv2d_31': <keras.layers.convolutional.Conv2D at 0x1f735110da0>,\n",
       " 'conv2d_34': <keras.layers.convolutional.Conv2D at 0x1f73538f898>,\n",
       " 'conv2d_39': <keras.layers.convolutional.Conv2D at 0x1f7357e57b8>,\n",
       " 'conv2d_40': <keras.layers.convolutional.Conv2D at 0x1f7358d0b00>,\n",
       " 'batch_normalization_31': <keras.layers.normalization.BatchNormalization at 0x1f7351afc50>,\n",
       " 'batch_normalization_34': <keras.layers.normalization.BatchNormalization at 0x1f7353cdeb8>,\n",
       " 'batch_normalization_39': <keras.layers.normalization.BatchNormalization at 0x1f735849a90>,\n",
       " 'batch_normalization_40': <keras.layers.normalization.BatchNormalization at 0x1f73590ff98>,\n",
       " 'activation_31': <keras.layers.core.Activation at 0x1f7351afeb8>,\n",
       " 'activation_34': <keras.layers.core.Activation at 0x1f7353f7358>,\n",
       " 'activation_39': <keras.layers.core.Activation at 0x1f7358a7ba8>,\n",
       " 'activation_40': <keras.layers.core.Activation at 0x1f73593a5c0>,\n",
       " 'mixed4': <keras.layers.merge.Concatenate at 0x1f7359c4358>,\n",
       " 'conv2d_45': <keras.layers.convolutional.Conv2D at 0x1f77f5be898>,\n",
       " 'batch_normalization_45': <keras.layers.normalization.BatchNormalization at 0x1f77f648128>,\n",
       " 'activation_45': <keras.layers.core.Activation at 0x1f77f620da0>,\n",
       " 'conv2d_46': <keras.layers.convolutional.Conv2D at 0x1f77f681da0>,\n",
       " 'batch_normalization_46': <keras.layers.normalization.BatchNormalization at 0x1f77f6e7f28>,\n",
       " 'activation_46': <keras.layers.core.Activation at 0x1f77f6e7a58>,\n",
       " 'conv2d_42': <keras.layers.convolutional.Conv2D at 0x1f735a86898>,\n",
       " 'conv2d_47': <keras.layers.convolutional.Conv2D at 0x1f77f79aef0>,\n",
       " 'batch_normalization_42': <keras.layers.normalization.BatchNormalization at 0x1f735af0438>,\n",
       " 'batch_normalization_47': <keras.layers.normalization.BatchNormalization at 0x1f77f7d7940>,\n",
       " 'activation_42': <keras.layers.core.Activation at 0x1f735af07f0>,\n",
       " 'activation_47': <keras.layers.core.Activation at 0x1f77f7d7ac8>,\n",
       " 'conv2d_43': <keras.layers.convolutional.Conv2D at 0x1f77eb3de80>,\n",
       " 'conv2d_48': <keras.layers.convolutional.Conv2D at 0x1f77f8e6320>,\n",
       " 'batch_normalization_43': <keras.layers.normalization.BatchNormalization at 0x1f77eba6f60>,\n",
       " 'batch_normalization_48': <keras.layers.normalization.BatchNormalization at 0x1f77f89ee80>,\n",
       " 'activation_43': <keras.layers.core.Activation at 0x1f77ebce080>,\n",
       " 'activation_48': <keras.layers.core.Activation at 0x1f77f922b38>,\n",
       " 'average_pooling2d_5': <keras.layers.pooling.AveragePooling2D at 0x1f77fa3de48>,\n",
       " 'conv2d_41': <keras.layers.convolutional.Conv2D at 0x1f735994dd8>,\n",
       " 'conv2d_44': <keras.layers.convolutional.Conv2D at 0x1f77ee9d390>,\n",
       " 'conv2d_49': <keras.layers.convolutional.Conv2D at 0x1f77f94a588>,\n",
       " 'conv2d_50': <keras.layers.convolutional.Conv2D at 0x1f77fa0def0>,\n",
       " 'batch_normalization_41': <keras.layers.normalization.BatchNormalization at 0x1f735a02668>,\n",
       " 'batch_normalization_44': <keras.layers.normalization.BatchNormalization at 0x1f77f187748>,\n",
       " 'batch_normalization_49': <keras.layers.normalization.BatchNormalization at 0x1f77f986ef0>,\n",
       " 'batch_normalization_50': <keras.layers.normalization.BatchNormalization at 0x1f77fa79a20>,\n",
       " 'activation_41': <keras.layers.core.Activation at 0x1f735a25208>,\n",
       " 'activation_44': <keras.layers.core.Activation at 0x1f77f187cc0>,\n",
       " 'activation_49': <keras.layers.core.Activation at 0x1f77f9b73c8>,\n",
       " 'activation_50': <keras.layers.core.Activation at 0x1f77faa6080>,\n",
       " 'mixed5': <keras.layers.merge.Concatenate at 0x1f9014717f0>,\n",
       " 'conv2d_55': <keras.layers.convolutional.Conv2D at 0x1f9017ff710>,\n",
       " 'batch_normalization_55': <keras.layers.normalization.BatchNormalization at 0x1f90183fc50>,\n",
       " 'activation_55': <keras.layers.core.Activation at 0x1f90183fc88>,\n",
       " 'conv2d_56': <keras.layers.convolutional.Conv2D at 0x1f9018f5e80>,\n",
       " 'batch_normalization_56': <keras.layers.normalization.BatchNormalization at 0x1f901934e80>,\n",
       " 'activation_56': <keras.layers.core.Activation at 0x1f901934a58>,\n",
       " 'conv2d_52': <keras.layers.convolutional.Conv2D at 0x1f901558320>,\n",
       " 'conv2d_57': <keras.layers.convolutional.Conv2D at 0x1f9019b8630>,\n",
       " 'batch_normalization_52': <keras.layers.normalization.BatchNormalization at 0x1f90159bf28>,\n",
       " 'batch_normalization_57': <keras.layers.normalization.BatchNormalization at 0x1f901a42f60>,\n",
       " 'activation_52': <keras.layers.core.Activation at 0x1f9015c3780>,\n",
       " 'activation_57': <keras.layers.core.Activation at 0x1f901a7bac8>,\n",
       " 'conv2d_53': <keras.layers.convolutional.Conv2D at 0x1f901654320>,\n",
       " 'conv2d_58': <keras.layers.convolutional.Conv2D at 0x1f901aae518>,\n",
       " 'batch_normalization_53': <keras.layers.normalization.BatchNormalization at 0x1f90168f6d8>,\n",
       " 'batch_normalization_58': <keras.layers.normalization.BatchNormalization at 0x1f901ae2dd8>,\n",
       " 'activation_53': <keras.layers.core.Activation at 0x1f90168fc50>,\n",
       " 'activation_58': <keras.layers.core.Activation at 0x1f901ae2b00>,\n",
       " 'average_pooling2d_6': <keras.layers.pooling.AveragePooling2D at 0x1f901c5b9b0>,\n",
       " 'conv2d_51': <keras.layers.convolutional.Conv2D at 0x1f9014fa048>,\n",
       " 'conv2d_54': <keras.layers.convolutional.Conv2D at 0x1f901717828>,\n",
       " 'conv2d_59': <keras.layers.convolutional.Conv2D at 0x1f901b93e48>,\n",
       " 'conv2d_60': <keras.layers.convolutional.Conv2D at 0x1f901c5bf98>,\n",
       " 'batch_normalization_51': <keras.layers.normalization.BatchNormalization at 0x1f9014d7be0>,\n",
       " 'batch_normalization_54': <keras.layers.normalization.BatchNormalization at 0x1f90179f0b8>,\n",
       " 'batch_normalization_59': <keras.layers.normalization.BatchNormalization at 0x1f901bd2f28>,\n",
       " 'batch_normalization_60': <keras.layers.normalization.BatchNormalization at 0x1f901cc2320>,\n",
       " 'activation_51': <keras.layers.core.Activation at 0x1f901533e80>,\n",
       " 'activation_54': <keras.layers.core.Activation at 0x1f9017d7cc0>,\n",
       " 'activation_59': <keras.layers.core.Activation at 0x1f901c35f60>,\n",
       " 'activation_60': <keras.layers.core.Activation at 0x1f901d1df98>,\n",
       " 'mixed6': <keras.layers.merge.Concatenate at 0x1f901d505c0>,\n",
       " 'conv2d_65': <keras.layers.convolutional.Conv2D at 0x1f9020a9c50>,\n",
       " 'batch_normalization_65': <keras.layers.normalization.BatchNormalization at 0x1f9021175f8>,\n",
       " 'activation_65': <keras.layers.core.Activation at 0x1f90217eeb8>,\n",
       " 'conv2d_66': <keras.layers.convolutional.Conv2D at 0x1f90219fa58>,\n",
       " 'batch_normalization_66': <keras.layers.normalization.BatchNormalization at 0x1f9021dfeb8>,\n",
       " 'activation_66': <keras.layers.core.Activation at 0x1f90220c358>,\n",
       " 'conv2d_62': <keras.layers.convolutional.Conv2D at 0x1f901e39400>,\n",
       " 'conv2d_67': <keras.layers.convolutional.Conv2D at 0x1f902266fd0>,\n",
       " 'batch_normalization_62': <keras.layers.normalization.BatchNormalization at 0x1f901e78748>,\n",
       " 'batch_normalization_67': <keras.layers.normalization.BatchNormalization at 0x1f9022d0e10>,\n",
       " 'activation_62': <keras.layers.core.Activation at 0x1f901e78cc0>,\n",
       " 'activation_67': <keras.layers.core.Activation at 0x1f9022d0a20>,\n",
       " 'conv2d_63': <keras.layers.convolutional.Conv2D at 0x1f901eff4e0>,\n",
       " 'conv2d_68': <keras.layers.convolutional.Conv2D at 0x1f90234fa90>,\n",
       " 'batch_normalization_63': <keras.layers.normalization.BatchNormalization at 0x1f901f65ac8>,\n",
       " 'batch_normalization_68': <keras.layers.normalization.BatchNormalization at 0x1f9023bbef0>,\n",
       " 'activation_63': <keras.layers.core.Activation at 0x1f901fc2be0>,\n",
       " 'activation_68': <keras.layers.core.Activation at 0x1f90241ff28>,\n",
       " 'average_pooling2d_7': <keras.layers.pooling.AveragePooling2D at 0x1f9025059e8>,\n",
       " 'conv2d_61': <keras.layers.convolutional.Conv2D at 0x1f901d504a8>,\n",
       " 'conv2d_64': <keras.layers.convolutional.Conv2D at 0x1f901fea630>,\n",
       " 'conv2d_69': <keras.layers.convolutional.Conv2D at 0x1f902446978>,\n",
       " 'conv2d_70': <keras.layers.convolutional.Conv2D at 0x1f902537518>,\n",
       " 'batch_normalization_61': <keras.layers.normalization.BatchNormalization at 0x1f901d85fd0>,\n",
       " 'batch_normalization_64': <keras.layers.normalization.BatchNormalization at 0x1f902027fd0>,\n",
       " 'batch_normalization_69': <keras.layers.normalization.BatchNormalization at 0x1f902481ef0>,\n",
       " 'batch_normalization_70': <keras.layers.normalization.BatchNormalization at 0x1f9025727f0>,\n",
       " 'activation_61': <keras.layers.core.Activation at 0x1f901db15c0>,\n",
       " 'activation_64': <keras.layers.core.Activation at 0x1f902027d30>,\n",
       " 'activation_69': <keras.layers.core.Activation at 0x1f9024aa320>,\n",
       " 'activation_70': <keras.layers.core.Activation at 0x1f902599390>,\n",
       " 'mixed7': <keras.layers.merge.Concatenate at 0x1f90261fe48>,\n",
       " 'conv2d_73': <keras.layers.convolutional.Conv2D at 0x1f9027d06a0>,\n",
       " 'batch_normalization_73': <keras.layers.normalization.BatchNormalization at 0x1f90280fc50>,\n",
       " 'activation_73': <keras.layers.core.Activation at 0x1f90280fc88>,\n",
       " 'conv2d_74': <keras.layers.convolutional.Conv2D at 0x1f9028c2e80>,\n",
       " 'batch_normalization_74': <keras.layers.normalization.BatchNormalization at 0x1f902902fd0>,\n",
       " 'activation_74': <keras.layers.core.Activation at 0x1f902902a90>,\n",
       " 'conv2d_71': <keras.layers.convolutional.Conv2D at 0x1f9026376d8>,\n",
       " 'conv2d_75': <keras.layers.convolutional.Conv2D at 0x1f902a112e8>,\n",
       " 'batch_normalization_71': <keras.layers.normalization.BatchNormalization at 0x1f9026c1fd0>,\n",
       " 'batch_normalization_75': <keras.layers.normalization.BatchNormalization at 0x1f9029c7e48>,\n",
       " 'activation_71': <keras.layers.core.Activation at 0x1f9026880f0>,\n",
       " 'activation_75': <keras.layers.core.Activation at 0x1f902a49b00>,\n",
       " 'conv2d_72': <keras.layers.convolutional.Conv2D at 0x1f9026e65f8>,\n",
       " 'conv2d_76': <keras.layers.convolutional.Conv2D at 0x1f902a6e550>,\n",
       " 'batch_normalization_72': <keras.layers.normalization.BatchNormalization at 0x1f90274cb38>,\n",
       " 'batch_normalization_76': <keras.layers.normalization.BatchNormalization at 0x1f902ab8e80>,\n",
       " 'activation_72': <keras.layers.core.Activation at 0x1f9027a8c50>,\n",
       " 'activation_76': <keras.layers.core.Activation at 0x1f902ab8a90>,\n",
       " 'max_pooling2d_4': <keras.layers.pooling.MaxPooling2D at 0x1f902b66d68>,\n",
       " 'mixed8': <keras.layers.merge.Concatenate at 0x1f902b38ef0>,\n",
       " 'conv2d_81': <keras.layers.convolutional.Conv2D at 0x1f903ea2a90>,\n",
       " 'batch_normalization_81': <keras.layers.normalization.BatchNormalization at 0x1f903f0cb70>,\n",
       " 'activation_81': <keras.layers.core.Activation at 0x1f903f0c128>,\n",
       " 'conv2d_78': <keras.layers.convolutional.Conv2D at 0x1f902c2c748>,\n",
       " 'conv2d_82': <keras.layers.convolutional.Conv2D at 0x1f903f65b00>,\n",
       " 'batch_normalization_78': <keras.layers.normalization.BatchNormalization at 0x1f902c94390>,\n",
       " 'batch_normalization_82': <keras.layers.normalization.BatchNormalization at 0x1f903fcfe10>,\n",
       " 'activation_78': <keras.layers.core.Activation at 0x1f902c94748>,\n",
       " 'activation_82': <keras.layers.core.Activation at 0x1f903fcfb38>,\n",
       " 'conv2d_79': <keras.layers.convolutional.Conv2D at 0x1f902cf0dd8>,\n",
       " 'conv2d_80': <keras.layers.convolutional.Conv2D at 0x1f902ddbc88>,\n",
       " 'conv2d_83': <keras.layers.convolutional.Conv2D at 0x1f904083e80>,\n",
       " 'conv2d_84': <keras.layers.convolutional.Conv2D at 0x1f9041ce2e8>,\n",
       " 'average_pooling2d_8': <keras.layers.pooling.AveragePooling2D at 0x1f904209d30>,\n",
       " 'conv2d_77': <keras.layers.convolutional.Conv2D at 0x1f902b38d68>,\n",
       " 'batch_normalization_79': <keras.layers.normalization.BatchNormalization at 0x1f902d59c50>,\n",
       " 'batch_normalization_80': <keras.layers.normalization.BatchNormalization at 0x1f903e19630>,\n",
       " 'batch_normalization_83': <keras.layers.normalization.BatchNormalization at 0x1f9040c1fd0>,\n",
       " 'batch_normalization_84': <keras.layers.normalization.BatchNormalization at 0x1f904186e48>,\n",
       " 'conv2d_85': <keras.layers.convolutional.Conv2D at 0x1f904259710>,\n",
       " 'batch_normalization_77': <keras.layers.normalization.BatchNormalization at 0x1f902c0acf8>,\n",
       " 'activation_79': <keras.layers.core.Activation at 0x1f902d59c88>,\n",
       " 'activation_80': <keras.layers.core.Activation at 0x1f903e7eeb8>,\n",
       " 'activation_83': <keras.layers.core.Activation at 0x1f9040c1a90>,\n",
       " 'activation_84': <keras.layers.core.Activation at 0x1f9041afac8>,\n",
       " 'batch_normalization_85': <keras.layers.normalization.BatchNormalization at 0x1f90429e390>,\n",
       " 'activation_77': <keras.layers.core.Activation at 0x1f902c0af60>,\n",
       " 'mixed9_0': <keras.layers.merge.Concatenate at 0x1f903f2b2e8>,\n",
       " 'concatenate_1': <keras.layers.merge.Concatenate at 0x1f90422f550>,\n",
       " 'activation_85': <keras.layers.core.Activation at 0x1f9042f6f98>,\n",
       " 'mixed9': <keras.layers.merge.Concatenate at 0x1f9042f6940>,\n",
       " 'conv2d_90': <keras.layers.convolutional.Conv2D at 0x1f9045e6ac8>,\n",
       " 'batch_normalization_90': <keras.layers.normalization.BatchNormalization at 0x1f9046f6c88>,\n",
       " 'activation_90': <keras.layers.core.Activation at 0x1f904753be0>,\n",
       " 'conv2d_87': <keras.layers.convolutional.Conv2D at 0x1f9043ee860>,\n",
       " 'conv2d_91': <keras.layers.convolutional.Conv2D at 0x1f90477b6d8>,\n",
       " 'batch_normalization_87': <keras.layers.normalization.BatchNormalization at 0x1f904452400>,\n",
       " 'batch_normalization_91': <keras.layers.normalization.BatchNormalization at 0x1f9047bec50>,\n",
       " 'activation_87': <keras.layers.core.Activation at 0x1f9044527b8>,\n",
       " 'activation_91': <keras.layers.core.Activation at 0x1f9047bec88>,\n",
       " 'conv2d_88': <keras.layers.convolutional.Conv2D at 0x1f9044b1e48>,\n",
       " 'conv2d_89': <keras.layers.convolutional.Conv2D at 0x1f9045cd358>,\n",
       " 'conv2d_92': <keras.layers.convolutional.Conv2D at 0x1f904842c88>,\n",
       " 'conv2d_93': <keras.layers.convolutional.Conv2D at 0x1f904936748>,\n",
       " 'average_pooling2d_9': <keras.layers.pooling.AveragePooling2D at 0x1f9049f8be0>,\n",
       " 'conv2d_86': <keras.layers.convolutional.Conv2D at 0x1f9042f6d30>,\n",
       " 'batch_normalization_88': <keras.layers.normalization.BatchNormalization at 0x1f9045416a0>,\n",
       " 'batch_normalization_89': <keras.layers.normalization.BatchNormalization at 0x1f90460e710>,\n",
       " 'batch_normalization_92': <keras.layers.normalization.BatchNormalization at 0x1f9048ad5f8>,\n",
       " 'batch_normalization_93': <keras.layers.normalization.BatchNormalization at 0x1f90499aac8>,\n",
       " 'conv2d_94': <keras.layers.convolutional.Conv2D at 0x1f904a497f0>,\n",
       " 'batch_normalization_86': <keras.layers.normalization.BatchNormalization at 0x1f904367b70>,\n",
       " 'activation_88': <keras.layers.core.Activation at 0x1f904517d30>,\n",
       " 'activation_89': <keras.layers.core.Activation at 0x1f902c69668>,\n",
       " 'activation_92': <keras.layers.core.Activation at 0x1f9048adb70>,\n",
       " 'activation_93': <keras.layers.core.Activation at 0x1f90499a4e0>,\n",
       " 'batch_normalization_94': <keras.layers.normalization.BatchNormalization at 0x1f904a8f550>,\n",
       " 'activation_86': <keras.layers.core.Activation at 0x1f9043caf28>,\n",
       " 'mixed9_1': <keras.layers.merge.Concatenate at 0x1f90468fb70>,\n",
       " 'concatenate_2': <keras.layers.merge.Concatenate at 0x1f9049f8c50>,\n",
       " 'activation_94': <keras.layers.core.Activation at 0x1f904b193c8>,\n",
       " 'mixed10': <keras.layers.merge.Concatenate at 0x1f904ae8d68>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dict= dict([(layer.name, layer) for layer in model.layers]) #Creates a dictionary that maps layer names to layer instances\n",
    "layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "loss=K.variable(0.) #We'll define the loss by adding layer contributions to this scalar variable\n",
    "for layer_name in layer_contributions:\n",
    "    coeff= layer_contributions[layer_name]\n",
    "    activation= layers_dict[layer_name].output #Retrieves the layer’s output\n",
    "\n",
    "    scaling= K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss += coeff*K.sum(K.square(activation[:, 2: -2, 2: -2, :]))/scaling  #Adds the L2 norm of the features of a layer\n",
    "                                        #to the loss. We avoid border artifacts by only involving nonborder pixels in the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can set up the gradient-ascent process.\n",
    "\n",
    "### Gradient-Ascent Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream= model.input #This tensor holds the generated image: the dream\n",
    "\n",
    "grads= K.gradients(loss, dream)[0]   #Computes the gradients of the dream with regard to the loss\n",
    "\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7) #Normalizes the gradients (important trick)\n",
    "\n",
    "outputs= [loss, grads]                                 #Sets up a Keras function to retrieve the value of\n",
    "fetch_loss_and_grads= K.function([dream], outputs)     #the loss and gradients, given an input image\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs= fetch_loss_and_grads([x])\n",
    "    loss_value= outs[0]\n",
    "    grad_value= outs[1]\n",
    "    return loss_value, grad_value\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss= None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grads_value= eval_loss_and_graphs(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('... Loss Value at',i,': ', loss_value)\n",
    "        x+= step+grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally: the actual DeepDream algorithm. First, we define a list of scales (also called octaves) at which to process the images. Each successive scale is larger than the previous one by a factor of 1.4 (it’s 40% larger): we start by processing a small image and then increasingly scale it up.\n",
    "![image](https://user-images.githubusercontent.com/13174586/51905645-c28dc800-23e7-11e9-89e7-30ffd9fc6e47.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
